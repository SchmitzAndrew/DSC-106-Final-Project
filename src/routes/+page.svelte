<script>
  import { base } from '$app/paths';
  import * as d3 from 'd3';
  import { onMount } from 'svelte';

  import PredictedDistribution from '../components/PredictedDistribution.svelte';
  import ActualDistribution from '../components/ActualDistribution.svelte';
  import PlayerPredicter from '../components/PlayerPredicter.svelte';



  
</script>




  <section class="box" data-index={0}>
    <div class="text-content">
      <h2>Maximum Likelihood Estimation</h2>
      <p>Have you ever wanted to be able to predict things in nature? Whether it is the species of an animal,
        roll of a die,  or anything that has a specific class. It might surprise you that you are able to 
        do this using some math you may already know. But there is one issue when it comes to predicting 
        outcomes of nature. A major drawback when it comes to predicting things in nature is that it‚Äôs rare 
        that we have the true distribution of where the data came from. Luckily there is a solution to this. 
        One approach that data scientists use is they assume the data comes from some parametric density given the data is continuous.</p>
        <img src="{base}/gaussian.png" alt="A graph of a normal distribution" class="mle-image"/>
      <!-- More paragraphs as needed -->
    </div>
  </section>

  <section class="box" data-index={1}>
    <div class="text-content">
      <h2>Thinking as a Data Scientist</h2>
      <p>To start Data Scientists first make an assumption that being that the true, underlying density has a 
        certain form. A common assumption used is that the true distribution is Gaussian or also known as 
        normal distribution. With this assumption there are two parameters that determine the shape of the 
        distribution being mean and standard deviation. The mean controls the center and the standard 
        deviation controls the width of the distribution. But because we do not know the true density we 
        have to make assumptions on the parameters. This allows us to make an estimation to what the true 
        distribution may look like.</p>
      <!-- More paragraphs as needed -->
    </div>
  </section>


  <section class="box" data-index={2}>
    <div class="text-content">
      <h2>Looking at Real Data</h2>
      <p>Below is data sampled from the NBA showing the average number of rebounds center and points guards gotten
        in a random NBA season. As you can see you are able to input your own Means and Standard deviations. You will
        be playing the role as a Data Scientist in trying to make a model distribution that best fits the 
        data. Go ahead and see how well you do!!! Then whenever you feel confident in your answer, compare 
        yours with the best possible fit.</p>
        <img src="{base}/nba-logo-transparent.png" alt="The nba logo" class="nba-image"/>
      <!-- More paragraphs as needed -->
    </div>
  </section>

  <section class="box" data-index={3}>
    <div class="text-content">
      <p> In the following visualization, you will be able to find the rebound percentage for point guards in purple and 
      the rebound percentage for center players in light blue. By looking at their distribution in the x-axis,
      try to guess how their best fit Gaussian distributions would look like.</p>
      <PredictedDistribution />
      <!-- More paragraphs as needed -->
    </div>
  </section>

  


  <section class="box" data-index={4}>
    <div class="text-content">
      <p> Let's see how your prediction for the distributions of these two positions compares to the actual distributions. Click on "Show Distribution" to reveal
        the true distributions.</p>
        <ActualDistribution />
      <!-- More paragraphs as needed -->
    </div>
  </section>

  




 

  <section class="box" data-index={5}>
    <div class="text-content">
      <p> As you messed around with different parameter inputs you may have observed some inputs gave better
        results than others. Is there a way to quantify this without guessing and checking? Luckily ,
         some clever people have figured out how to quantify this step. First
         is to assume the data are all sampled independently. Next let's say that P(xi, Mu, Sigma) is
          the probability of seeing xi, a single data point, with the parameters Mu and Sigma. Then P(x1, Mu, Sigma) *  P(x2, Mu, Sigma) *... * P(xn, Mu, Sigma) 
          tells us the likelihood of seeing x1, x2,..., xn, the entire data set, at the sametime. We can think of this as a function of Mu and Sigma. 
          The likelihood function takes in a Mu and Sigma and returns the probability that data was generated by the inputted Mu and Sigma.</p>
      <!-- More paragraphs as needed -->
    </div>
  </section>

  <section class="box" data-index={6}>
    <div class="text-content">
      <p> The goal of creating this likelihood function is to maximize it by finding the Mu and Sigma that 
        gives us the highest likelihood. One way some of you all know is by the use of good old calculus. Which is to take the partial derivatives of the likelihood function for Mu and Sigma, set them to 0 then 
        solve for them.</p>
      <!-- More paragraphs as needed -->
    </div>
  </section>

  <section class="box" data-index={7}>
    <div class="text-content">
      <p>
        After you get through all the math you are left with the equations below. For those who are interested in the full derivations you can view them
        <a
          href="https://medium.com/swlh/gaussian-distribution-and-maximum-likelihood-estimate-method-step-by-step-e4f6014fa83e"
          >here.</a>
      </p>
      <img src="{base}/equations.png" alt="Equations for MLE" class="mle-image"/>
      <!-- More paragraphs as needed -->
    </div>
  </section>

  <section class="box" data-index={8}>
    <div class="text-content">
      <p> The mu maximum likelihood estimator is just the mean of the sampled data. And the Sigma maximum likelihood estimator is the standard deviation of data but using the mu mle.
        By calculating these parameters from the data we can now fit a gaussian that best fits our data.
    </p>
      <!-- More paragraphs as needed -->
    </div>
  </section>
  
  <section class="box" data-index={9}>
    <div class="text-content">
      <p> Ok but I know you are wondering how does this help you predict nature. 
        Well we can use this to estimate probability densities then by using Bayes Theorem. 
        But how? Well, back to our data set of centers and point guards. We will create a simple way  
        to predict whether a NBA player is a point guard or center by the number of rebounds. 
        We will create two separate fitted gaussians for each position like you seen before. 
        So we will have P(X| Y = PG) and P(X| Y = Center). Then multiply each of those distributions
        by P(Y=PG) and P(Y=Center) respectively. Now with those gaussians how would we predict the 
        position of a player? Well all you need to do is compare P(X| Y = PG)P(Y=PG) and P(X| Y = Center)P(Y=Center).
        Which ever has the higher probability then that will be the predictions. 
      </p>
      <!-- More paragraphs as needed -->
    </div>
  </section>

  <section class="box" data-index={10}>
    <div class="text-content">
      <p> Try it yourself below. We already created the P(X| Y = PG)P(Y=PG) and P(X| Y = Center)P(Y=Center) distributions
        for you. All you have to do is input the average rebounds of a center or point guard you know or look
       one up. Then click submit and see the predictions. Did we get it right?
      </p>
      <PlayerPredicter />
      <!-- More paragraphs as needed -->
    </div>
  </section>




<section class="box" data-index={11}>
  <div class="text-content">
    <p> As you can see we did not use some sort of magic or insane algorithm to create our predictions.
      We simply used MLE to estimate probability densities so we could use Bayes Theorem to find the probabilities
      of whether the player with the inputted rebound percentage was a center or point guard and chose the higher probability
      to make our prediction.
    </p>
    <!-- More paragraphs as needed -->
  </div>
</section>

  <section class="box" data-index={12}>
    <div class="text-content">
      <p> Some of you who are more familiar with statistics and math may wonder why would someone 
        estimate the densities P(ùë• | ùëå = 0) and P(ùë• | ùëå = 1) using a parametric approach instead of 
        using a non-parametric approach like using histograms. Well one reason is that histograms 
        suffer from the curse of dimensionality meaning when there are more dimensions (aka. features) 
        histograms will require significantly much more data to have a good fit. Meaning if the sample 
        is low and you can make a fair assumption about the underlying true density of the data then a 
        parametric approach might suit you better. Predicting nature is a very complicated process since 
        we will never fully understand how to quanitfy all the random interactions that happens that sum up
        into in a single outcome or outcomes. But by having a variety of tools like MLE it allows 
        us to predict nature the best we can and perhaps one day we may get really close to predict nature.
      </p>
      <!-- More paragraphs as needed -->
    </div>
  </section>

  <footer class="footer">
    <p>Created by Hector Gallo, Andrew Schmitz, and Diego Silva</p>
    <a href="https://github.com/SchmitzAndrew/DSC-106-Final-Project" target="_blank">
      <img src="{base}/github-logo.png" alt="GitHub Logo" class="github-logo"/>
    </a>
  </footer>




<style>
  .box.in-view {
    transform: scale(1);
  }


  .text-content h2 {
    font-size: 40px;
    margin-bottom: 16px;
    text-align: center;
  }
  .text-content p {
    font-size: 30px; /* Adjust the size as needed */
    padding: 0 20px; /* Add some padding around the text */
  }
  .mle-image {
    max-width: 50%; /* Ensures the image is not wider than the container */
    height: auto; /* Maintains the aspect ratio of the image */
    display: block; /* Renders the image as a block-level element */
    margin: 0 auto; /* Centers the image within the section */
    border-radius: 10px; /* Adds gentle rounding to the image */
  }
  .nba-image {
    max-width: 15%; /* Ensures the image is not wider than the container */
    height: auto; /* Maintains the aspect ratio of the image */
    display: block; /* Renders the image as a block-level element */
    margin: 0 auto; /* Centers the image within the section */
    border-radius: 10px; /* Adds gentle rounding to the image */
  }
.box {
  margin-bottom: 2rem; /* Adds space between sections */
  padding: 1rem; /* Additional padding within each section */
}

/* Remove the last-child margin-bottom if not needed */
.box:last-child {
  margin-bottom: 0;
}

.footer {
    background-color: #333; /* Dark background for the footer */
    color: #fff; /* Light text color for contrast */
    text-align: center; /* Center the footer content */
    padding: 20px 0; /* Add some padding above and below the content */
    font-size: 16px; /* Adjust the font size as needed */
    border-top: 4px solid #444; /* Add a top border for a bit of visual separation */
  }

  .footer a {
    color: #ddd; /* Slightly lighter color for links for contrast */
    text-decoration: none; /* Remove underline from links */
    margin: 0 10px; /* Add some horizontal spacing between links */
  }

  .footer a:hover {
    color: #fff; /* Change link color on hover for a visual feedback */
  }

  .footer .github-logo {
    width: 24px; /* Adjust the size of the GitHub logo */
    vertical-align: middle; /* Align the logo vertically with the text */
    margin-right: 8px; /* Add some spacing to the right of the logo */
    border-radius: 20%; /* Make the logo circular */    
  }
  
  /* Full-page background color and font style adjustments */
  body {
    background-color: #f0f0f0; /* Light grey background for the entire page */
    font-family: 'Roboto', sans-serif; /* Use Roboto font for all text */
  }
</style>
