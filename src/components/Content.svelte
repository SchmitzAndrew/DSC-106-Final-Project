<script>
  import { onMount } from "svelte";
  import katex from "katex";
  import "katex/dist/katex.min.css";

  let meanContainer;
  let stdDevContainer;

  onMount(() => {
    const meanLatex =
      "\\mu_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} x^{(i)}";
    const stdDevLatex =
      "\\sigma_{\\text{MLE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( x^{(i)} - \\mu_{\\text{MLE}} \\right)^2}";

    katex.render(meanLatex, meanContainer);
    katex.render(stdDevLatex, stdDevContainer);
  });
</script>

```
<main>
  <p>
    As you can see some estimations of the parameters seem to fit the data much
    better than others. Data Scientists have figured how to quantify this step
    to find the “best fit”. First step is to assume the data are all sampled
    independently. Next let's say that p(xi, mu, sigma) is the probability of
    seeing xi with the parameters mu and sigma. Then p(x1, mu, sigma) * p(x2,
    mu, sigma) *... * p(xn, mu, sigma) tells us the likelihood of seeing X1,
    x2,..., xn at the sametime. We can think of this as a function of mu and
    sigma. The likelihood function takes in a mu and sigma and returns the
    probability that data was generated by the inputted mu and sigma.
  </p>
  <p>
    The goal of creating this likelihood function is to maximize it by finding
    the mu and sigma that gives us the highest likelihood. One way is to take
    the partial derivatives, set them to 0 then solve for mu and sigma.
  </p>

  <p>
    Which gives us once all the math is done. If you want to look for an exact
    derivation you can look
    <a
      href="https://medium.com/swlh/gaussian-distribution-and-maximum-likelihood-estimate-method-step-by-step-e4f6014fa83e"
      >here</a
    >
  </p>

  <div>
    <div class="formula">
      <span bind:this={meanContainer}></span>
    </div>
    <div class="formula">
      <span bind:this={stdDevContainer}></span>
    </div>
  </div>

  <p>
    By calculating these parameters from the data we can now fit a gaussian that
    best fits our data.
  </p>

  <p>
    Why would someone estimate the densities 𝑝𝑋 (𝑥 | 𝑌 = 0) and 𝑝𝑋 (𝑥 | 𝑌 = 1)
    using parametrically instead of using a non-parametric approach like using
    histograms. Well one reason is that histograms suffer from the curse of
    dimensionality meaning when there are more dimensions (aka. features)
    histograms will require significantly much more data to have a good fit.
    Meaning if the sample is low and you can make a fair assumption about the
    underlying true density of the data then a parametric approach might suit
    you better.
  </p>
</main>

<style>
  .formula {
    margin-bottom: 1rem;
  }
</style>
