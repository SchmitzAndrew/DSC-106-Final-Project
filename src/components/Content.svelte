<script>
  import { onMount } from "svelte";
  import katex from "katex";
  import "katex/dist/katex.min.css";

  let meanContainer;
  let stdDevContainer;

  onMount(() => {
    const meanLatex =
      "\\mu_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} x^{(i)}";
    const stdDevLatex =
      "\\sigma_{\\text{MLE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( x^{(i)} - \\mu_{\\text{MLE}} \\right)^2}";

    katex.render(meanLatex, meanContainer);
    katex.render(stdDevLatex, stdDevContainer);
  });
</script>

<main>
  <p>
    As you messed around with different parameter inputs you may have observed some inputs gave better
       results than others. Is there a way to quantify this without guessing and checking? Luckily ,
        Data Scientists have figured out how to quantify this step to find the “best fit”. First step 
        is to assume the data are all sampled independently. Next let's say that p(xi, mu, sigma) is
         the probability of seeing xi with the parameters mu and sigma. Then p(x1, mu, sigma) *  p(x2, mu, sigma) *... * p(xn, mu, sigma) 
         tells us the likelihood of seeing X1, x2,..., xn at the sametime. We can think of this as a function of mu and sigma. 
         The likelihood function takes in a mu and sigma and returns the probability that data was generated by the inputted mu and sigma.
  </p>
  <p>
    The goal of creating this likelihood function is to maximize it by finding the mu and sigma that 
      gives us the highest likelihood. One way is to take the partial derivatives, set them to 0 then 
      solve for mu and sigma.
  </p>

  <p>
    After you get through all the matches you are left with the equations below. For those who are interested in the full derivations you can view them
    <a
      href="https://medium.com/swlh/gaussian-distribution-and-maximum-likelihood-estimate-method-step-by-step-e4f6014fa83e"
      >here.</a
    >
  </p>

  <div>
    <div class="formula">
      <span bind:this={meanContainer}></span>
    </div>
    <div class="formula">
      <span bind:this={stdDevContainer}></span>
    </div>
  </div>

  <p>
    The mu maximum likelihood estimator is just the mean of the sampled data. And the sigma maximum likelihood estimator is the standard deviation of data but using the mu mle.
      By calculating these parameters from the data we can now fit a gaussian that best fits our data.
  </p>

  <p>
    Ok but I know you are wondering how this helps you predict things in nature. Well we can use this to estimate probability densities. But how? Well, our example of figuring how to predict whether a NBA player is a point guard or forward by the number of rebounds can give us insight on how it is done. We will create two separate fitted gaussians for each position. So we will have P(X| Y = PG) and P(X| Y = Forward). Then multiple each of those probabilities by P(Y=PG) and p(Y=Forward) respectively. Now with those gaussians how would we predict the position of a player? Well we will see which probability is greater at x.
  </p>

  <p>
    Some of you who are more familiar with statistics and math may wonder why would someone estimate the densities 𝑝𝑋 (𝑥 | 𝑌 = 0) and 𝑝𝑋 (𝑥 | 𝑌 = 1) using parametrically instead of using a non-parametric approach like using histograms. Well one reason is that histograms suffer from the curse of dimensionality meaning when there are more dimensions (aka. features) histograms will require significantly much more data to have a good fit. Meaning if the sample is low and you can make a fair assumption about the underlying true density of the data then a parametric approach might suit you better. Predicting nature is a very complicated process, one that we may never fully understand or be able to do but by having a variety of tools it allows us to do our best and perhaps one day we may get really close to predict nature.
  </p>
</main>

<style>
  .formula {
    margin-bottom: 1rem;
  }
  .text-content p {
    font-size: 40px; /* Adjust the size as needed */
    padding: 0 20px; /* Add some padding around the text */
  }
  
</style>
